---
layout: post
title: "Implementing RTC Correctly"
subtitle: "A Subtle Gradient-Propagation Bug and Its Fix"
date:   2026-01-26 11:41:56 +0800
categories: jekyll update
math: true
author: "Linzhi Wu, Junjie Xu"
citekey: "wu2026rtc"

---

## Introduction

Recent vision-language-action (VLA) models have shown impressive capability in robotic manipulation, but their high inference latency remains a major obstacle for real-time deployment. The Real-Time Chunking (RTC) framework addresses this issue by enabling asynchronous execution of action-chunked policies, reformulating real-time control as an inference-time inpainting problem. Without any retraining, RTC allows a policy to generate future action chunks while safely executing previously committed actions, achieving smooth and delay-robust behavior in both simulation and real-world robotic tasks.

While reproducing the results of this work and experimenting with its released implementation, I ran into an easy-to-make implementation pitfall that can meaningfully affect cross-chunk continuity and runtime behavior. Specifically, when implementing RTC’s per-step correction for stitching chunks, it is tempting to let gradients propagate through the learned vector field / flow policy $$v(\cdot)$$ via automatic differentiation. However, the intended update is to adjust the current latent/state to enforce consistency, rather than to differentiate through $$v$$ itself. In practice, allowing $$\nabla v$$ to participate changes the effective update rule, which can introduce discrepancies between the method described in the paper and its practical execution. In this post, I briefly summarize the key mechanism behind RTC and then focus on this gradient-handling pitfall, explaining its impact and a simple correction (stopping gradients through $$v$$) that better matches the intended algorithm.

## Preliminaries

### Action chunking and horizons

RTC considers an action-chunking policy $$\pi(A_t \mid o_t)$$, where
$$A_t = [a_t, a_{t+1}, \ldots, a_{t+H-1}]$$ is a chunk of $$H$$ future actions (the **prediction horizon**).  
At rollout time, only the first $$s \le H$$ actions are executed (the **execution horizon**). Longer $$s$$ improves temporal consistency but reduces reactivity; shorter $$s$$ increases the chance of discontinuities between chunks. 

### Flow policy sampling

RTC focuses on policies trained with conditional flow matching (diffusion policies can be converted to flow policies at inference time).  
A chunk is generated by sampling Gaussian noise and integrating the learned velocity field $$v_\pi$$ over flow time $$\tau \in [0,1)$$ with $$n$$ denoising steps:

$$
A^{\tau + \frac{1}{n}}_t = A^\tau_t + \frac{1}{n}\, v_\pi(A^\tau_t, o_t, \tau).
\tag{1}
$$

### Real-time constraint and inference delay

Let $$\Delta t$$ be the controller period and $$\delta$$ the wall-clock time to generate one chunk. RTC defines the **inference delay**

$$
d := \left\lfloor \frac{\delta}{\Delta t} \right\rfloor,
\tag{2}
$$

the number of control steps between receiving $$o_t$$ and having $$A_t$$ available. 
A naive asynchronous strategy can be real-time when $$d \le H - s$$, but it may cause a discontinuous switch at the chunk boundary because the policy cannot predict what happens during the delay window. 

### RTC as inpainting with soft masking 

RTC frames asynchronous chunking as an **inpainting** problem: while executing the previous chunk, the actions that are guaranteed to be executed due to delay are treated as “frozen,” and the rest of the new chunk is generated to be compatible with the previous one.  
To strengthen cross-chunk continuity, RTC uses **soft masking** weights $$W$$: the first $$d$$ overlapping actions receive weight 1, the non-overlapping tail receives weight 0, and the intermediate region decays smoothly from 1 to 0. The guided inference step is implemented via a vector–Jacobian product (VJP) computed with reverse-mode autodifferentiation, which is exactly where gradient-handling becomes subtle in practice. 

## The ΠGDM Correction Term and Differentiating with $$v$$ Treated as Constant

In RTC, the ΠGDM-style corrected vector field can be written as

$$
v_{\Pi\mathrm{GDM}}(A_t^\tau,o_t,\tau)
=
v(A_t^\tau,o_t,\tau)
+
\min\!\left(
\beta,\;
\frac{1-\tau}{\tau \cdot r_\tau^{2}}
\right)
\bigl(Y - \hat{A}_t^{1}\bigr)^\top \mathrm{diag}(W)\;
\frac{\partial A_t^{1}}{\partial A_t^\tau},
\tag{3}
$$

where

$$
\hat{A}_t^{1}=A_t^\tau+(1-\tau)\,v(A_t^\tau,o_t,\tau).
\tag{4}
$$

If we apply the chain rule strictly, then

$$
\frac{\partial \hat{A}_t^{1}}{\partial A_t^\tau}
=
I + (1-\tau)\frac{\partial v(A_t^\tau,o_t,\tau)}{\partial A_t^\tau}.
\tag{5}
$$

At first glance, it seems natural to use (5) when computing the VJP in (3).

However, in RTC’s runtime stitching (the inpainting/continuity view), it is often more appropriate to **treat $$v$$ as constant** (i.e., stop gradients through $$v$$) so that

$$
\frac{\partial \hat{A}_t^{1}}{\partial A_t^\tau} = I,
\tag{6}
$$

and the correction direction reduces to the most direct weighted residual:

$$
\min\!\left(
\beta,\;
\frac{1-\tau}{\tau \cdot r_\tau^{2}}
\right)
\bigl(Y - \hat{A}_t^{1}\bigr)^\top \mathrm{diag}(W).
\tag{7}
$$

### Why treat $$v$$ as constant in RTC?

The key point is that this guidance step is meant to enforce **cross-chunk continuity**—conceptually an inpainting constraint that pulls the current prediction $$A_t^{c1}$$ toward the target $$Y$$ in the overlap region (weighted by $$W$$). The guidance should update the sample/state $$A_t^\tau$$ to satisfy the constraint. In practice, an incorrect gradient path can **dampen the effective guidance signal**, making the resulting correction term too small to meaningfully steer the trajectory at chunk boundaries. When the guidance is underpowered, the sampler largely follows the unconditional flow, and cross-chunk continuity degrades because the constraint is not enforced strongly enough.

From an implementation perspective, $$v(A_t^\tau,o_t,\tau)$$ is produced by a neural network and is implicitly a function of the current state $$A_t^\tau$$. If one differentiates through this dependency, the guidance term is no longer a direct residual-based correction; instead, it becomes entangled with how the network output changes with its input. In RTC’s setting, the intended behavior is to treat $$v(A_t^\tau,o_t,\tau)$$ as a fixed function evaluation at the current step and use it only to construct the update for $$A_t^\tau$$, rather than propagating gradients through $$v$$ itself.

Here is code that is definitely correct for 1→0 flow integration (e.g., \\(\pi_{0}\\)). In `def denoiser()`, we apply `jax.lax.stop_gradient(v_t)` to ensure that `jax.vjp` does not backpropagate through the gradient of `v_t`.

```python
import jax
import jax.numpy as jnp
from typing import Callable


def pinv_corrected_velocity(
    v_t_fn: Callable, # ([ah ad], float) -> [ah ad]
    x_t: jax.Array, # [b ah ad]
    t: float,
    prefix_actions: jax.Array, # [b ah ad]
    inference_delay: int,
    prefix_attention_horizon: int,
    max_guidance_weight: float,
) -> jax.Array: # [b ah ad]
    @jax.vmap
    def _pinv_corrected_velocity(
        x_t: jax.Array, # [ah ad]
        y: jax.Array, # [ah ad]
    ) -> jax.Array: # [ah ad]
        def denoiser(x_t: jax.Array) -> tuple[jax.Array, jax.Array]:
            v_t = v_t_fn(x_t, t)
            x_0 = x_t - jax.lax.stop_gradient(v_t) * t
            return x_0, v_t

        x_0, vjp_fun, v_t = jax.vjp(denoiser, x_t, has_aux=True)
        error = (y - x_0) * get_prefix_weights(inference_delay, prefix_attention_horizon, prefix_actions.shape[1])[:, None]
        pinv_correction = vjp_fun(error)[0]
        # constants from paper
        inv_r2 = (t**2 + (1 - t) ** 2) / (t**2)
        c = jnp.nan_to_num(t / (1 - t), posinf=max_guidance_weight)
        guidance_weight = jnp.minimum(c * inv_r2, max_guidance_weight)
        return v_t - guidance_weight * pinv_correction

    return _pinv_corrected_velocity(x_t, prefix_actions)
```

## Experiment: What does correctly implemented RTC look like in practice?

We fine-tuned \\(\pi_{05}\\) on a dual-arm Flexiv robot to perform a LEGO sorting task. The figure below shows RTC results evaluated on dataset trajectories. 

- <span style="color: purple;">purple</span> denotes the actions generated by RTC,

- <span style="color: green;">green</span> denotes the corresponding actions from the previous action chunk used for guidance, 

- <span style="color: red;">red dashed</span> denotes the actions generated without RTC, 

- <span style="color: gray;">gray dashed</span> denotes the inference delay.

We use an action chunk length of 100. The RTC-related parameters in this test are: `inference_delay = 16`, `max_guidance_weight = 10`, and `execution_horizon = 64`.

Notably, the y-axis is measured in meters. The 14 dimensions in the plot correspond to \\((x, y, z, r_x, r_y, r_z, \\text{gripper}) \\times 2\\)

![]({{ "/img/correction,max_guidance=10.png" | relative_url }})

In contrast, using the same parameters and input data but simply removing `jax.lax.stop_gradient(v_t)` yields the following result:

![]({{ "/img/original,max_guidance=10.png" | relative_url }})

In addition, tuning `max_guidance_weight` suggests that `max_guidance_weight = 10` is better suited for the default \\(\pi_{05}\\) implementation. This smoothing idea is also discussed in Soare’s blog post.[^soare2025]

### max_guidance_weight = 5

![]({{ "/img/correction,max_guidance_weight=5.png" | relative_url }})

### max_guidance_weight = 100

![]({{ "/img/correction,max_guidance_weight=100.png" | relative_url }})

> ⚠️ **Important note / Limitations.**  
> These experiments were conducted under fairly limited conditions—only a small number of tasks and just a handful of episodes—though we tested multiple action parameterizations, including absolute end-effector poses, relative end-effector poses, relative joint angles, and absolute joint angles. Despite these limitations, we still believe the observations reported here are likely to generalize to other settings.

## Citation

Please feel free to cite this work as


```bibtex
@misc{ {{ page.citekey }},
  author = { {{ page.author }} },
  title  = { {{ page.title }} },
  year   = { {{ page.date | date: "%Y" }} },
  url    = { {{ page.url | absolute_url }} },
  note   = { Blog post }
}
```

[^soare2025]: Alexander Soare, [*Smooth-As-Butter Robot Policies*](https://alexander-soare.github.io/robotics/2025/08/05/smooth-as-butter-robot-policies.html), 2025 (blog post).
